{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "861b6d4a-0de6-42ba-97a5-beef1f82f292",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "# Projekt Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b301ae8-ceff-4dbf-8d04-75bb4eb52480",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "# Wprowadzenie\n",
    "\n",
    "Wykorzystując ten notatnik jako szablon zrealizuj projekt *Apache Spark* zgodnie z przydzielonym zestawem. \n",
    "\n",
    "Kilka uwag:\n",
    "\n",
    "* W szablonie nie wolno zmieniać paragrafów *markdown*, zawierających *instrukcje wykonania projektu*. <br>\n",
    "  Każdy taki paragraf jest oznaczony za pomocą `> Instrukcja wykonania projektu`\n",
    "* Istniejące paragrafy zawierające *kod* uzupełnij w razie potrzeby zgodnie z instrukcjami\n",
    "    - nie usuwaj ich\n",
    "    - modyfikuj je tylko w zakresie zgodnym z instrukcjami\n",
    "* Możesz dodawać nowe paragrafy zarówno zawierające kod jak i komentarze dotyczące tego kodu (markdown)\n",
    "* Nie możesz zmieniać kolejności paragrafów zawierających instrukcje. Notatnik ma mieć niezmienioną strukturę.\n",
    "* Utworzony notatnik musi być \"samowystarczalny\" w kontekście wykorzystywanej platformy przy założeniu dostepu do sieci.<br>\n",
    "  Nie może wykorzystywać komponentów, które: \n",
    "    -  nie są domyślnie dostepne w ramach platformy i/lub\n",
    "    -  nie są samodzielnie pobierane przez ten notatnik z oficjalnych repozytoriów (*Maven\").  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d12f1-1013-4c74-b6aa-686ccfcbdd5c",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "Poniżej w paragrafie markdown umieść numer oraz pełny tytuł przydzielonego zestawu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc4ff6-4d43-49ed-a0d1-8b6988eaec16",
   "metadata": {},
   "source": [
    "# Zestaw 15 - food-orders-restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b96ba57",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "W ponizszym paragrafie określ wartość zmiennej `input_dir` wskazującej miejsce, w którym znajdują się Twoje dane źródłowe (katalogi `datasource1` oraz `datasource4`).\n",
    "\n",
    "Ze zmiennej tej należy korzystać we wszystkich miejscach, w których odwołujemy się do danych źródłowych. Pełni ona rolę \"parametru\" naszego notatnika. \n",
    "\n",
    "***Pamiętaj*** aby przed rejestracją rozwiązania usunąć zawartość tej zmiennej. Osoba korzystająca z tego notatnika (np podczas oceny) samodzielnie ustawi sobie jej wartość na odpowiednią dla swojej konfiguracji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f00580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_dir = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e79a19",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "# Konteksty\n",
    "W poniższych paragrafach utwórz odpowiednie konteksty. Nasza aplikacja *Apache Spark* ma działać na klastrze z wykorzystaniem platformy *Hadoop (YARN)*. \n",
    "\n",
    "## Dodatkowe działania \n",
    "\n",
    "W zależności od potrzeb, wykorzystaj poniższe paragrafy do wykonania dodatkowych działań niezbędnych do utworzenia kontekstu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825442a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5bb04-67c7-4886-af9e-70fc5d334c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90c897f7",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "## Kontekst dla *DataFrame API*\n",
    "\n",
    "Wykorzystaj poniższy paragraf do utworzenia obiektu kontekstu dla *DataFrame API*. \n",
    "\n",
    "Nazwij go `spark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5200fcc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Projekt2-Zestaw15-FoodOrders\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.3.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da37060",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "## Kontekst dla *RDD API*\n",
    "\n",
    "Wykorzystaj poniższy paragraf do utworzenia obiektu kontekstu dla *RDD API*. \n",
    "\n",
    "Nazwij go `sc`. \n",
    "\n",
    "Utwórz go z powyżej utworzonego kontekstu. Dzięki temu całość notatnika będzie działała w ramach jednej aplikacji *Apache Spark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08580f5-a5d2-41bc-88ae-892ee11297c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14408872-4060-4a18-ba98-e1f60637d182",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "# Część 1 - Spark Core (*RDD API*)\n",
    "\n",
    "Pamiętaj o czystości Twojego API. Musi być ona zachowana od samego początku implementacji (utworzenia kontekstu), poprzez definicje źródeł i transformacje, aż do samego końca czyli wygenerowanie ostatecznego wyniku. \n",
    "\n",
    "Wykorzystaj odpowiedni kontekst utworzony w początkowej części notatnika."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d987ef2-c630-4552-9c96-099414ab7e7c",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "## Dane źródłowe\n",
    "\n",
    "W poniższych paragrafach utwórz dwa obiekty RDD, których zawartością będą Twoje dane źródłowe. \n",
    "\n",
    "Użyj nazw czytelnie odnoszących się zawartości Twoich danych źródłowych. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23aede-2679-4a99-a7be-a672f217490b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders_raw_rdd = sc.textFile(f\"{input_dir}/datasource1/*.csv\")\n",
    "\n",
    "header = \"order_id,restaurant_id,order_date,items_count,total_price_usd,payment_type,status\"\n",
    "orders_rdd = orders_raw_rdd.filter(lambda line: line != header and len(line.strip()) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd348c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "restaurants_raw_rdd = sc.textFile(f\"{input_dir}/datasource4/restaurants.csv\")\n",
    "\n",
    "header_restaurants = \"restaurant_id,name,city,country,cuisine\"\n",
    "restaurants_rdd = restaurants_raw_rdd.filter(lambda line: line != header_restaurants and len(line.strip()) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c06f9-e311-45e5-8206-e003882d1606",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "## Przetwarzanie - etap 1 - \"MapReduce\" \n",
    "\n",
    "### Przetwarzanie\n",
    "\n",
    "Twoim zadaniem jest utworzenie obiektu RDD o nazwie `mapreduce_RDD`, który w oparciu o: \n",
    "\n",
    "- utworzony kontekst, oraz\n",
    "- pierwszy ze źródłowych RDD (`datasource1`)\n",
    "\n",
    "będzie zawierał wynik zgodny z oczekiwaniami treści sekcji *Program MapReduce* dla Twojego zestawu. \n",
    "\n",
    "Wykorzystaj do tego celu poniższe paragrafy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06be3d4-cb35-48f9-8356-519ce31fbede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Etap 1: MapReduce - statystyki zamówień dla restauracji i typów płatności\n",
    "\n",
    "# Parsowanie, wczesna selekcja i projekcja\n",
    "def parse_and_filter_orders(line):\n",
    "\n",
    "    fields = line.split(',')\n",
    "    \n",
    "    if len(fields) < 7:\n",
    "        return None\n",
    "    \n",
    "    restaurant_id = fields[1].strip()\n",
    "    total_price = fields[4].strip()\n",
    "    payment_type = fields[5].strip()\n",
    "    status = fields[6].strip()\n",
    "    \n",
    "    # Wczesna selekcja\n",
    "    if status != \"Completed\":\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Wczesna projekcja\n",
    "        return (restaurant_id, payment_type, float(total_price))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Stosujemy parsowanie i filtrowanie\n",
    "parsed_orders = orders_rdd \\\n",
    "    .map(parse_and_filter_orders) \\\n",
    "    .filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c588e6-528e-4c80-9344-962e4eb3d9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map\n",
    "# Klucz: (restaurant_id, payment_type) Wartość: (total_price, count)\n",
    "\n",
    "mapped_orders = parsed_orders.map(\n",
    "    lambda x: ((x[0], x[1]), (x[2], 1))\n",
    ")\n",
    "\n",
    "# Agregacja wielopoziomowa z combinerem\n",
    "# Combiner: sumujemy na poziomie partycji\n",
    "def sum_combiner(a, b):\n",
    "    \"\"\"Sumuje pary (sum_price, count)\"\"\"\n",
    "    return (a[0] + b[0], a[1] + b[1])\n",
    "\n",
    "# Reduce: agregacja z combinerem\n",
    "aggregated_orders = mapped_orders.reduceByKey(sum_combiner)\n",
    "\n",
    "mapreduce_RDD = aggregated_orders.map(\n",
    "    lambda x: (\n",
    "        x[0][0],\n",
    "        x[0][1],\n",
    "        x[1][1],\n",
    "        round(x[1][0] / x[1][1], 6)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82343036-3e4e-4a16-80c1-6ab8e88d4600",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "### Wynik\n",
    "\n",
    "Pobierz liczbę elementów z `mapreduce_RDD`. \n",
    "\n",
    "Wykorzystaj do tego celu poniższy paragraf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec2161-b0da-43c3-acab-ad4b1366cf69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapreduce_count = mapreduce_RDD.count()\n",
    "print(f\"Liczba grup: {mapreduce_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5355ad-a6dd-4f1a-a25d-36692679e885",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "Posortuj rosnąco elementy `mapreduce_RDD` względem wszystkich wymaganych wynikowych atrybutów (w kolejności określonej w treści sekcji *Program MapReduce* dla Twojego zestawu). \n",
    "\n",
    "A następnie pobierz i wyświetl w oddzielnych liniach 10 pierwszych elementów. \n",
    "\n",
    "Wykorzystaj do tego celu poniższy paragraf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb73a60-dcbd-4888-8135-f4ad5f763889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sortujemy według wszystkich atrybutów: restaurant_id, payment_type\n",
    "sorted_mapreduce = mapreduce_RDD.sortBy(lambda x: (x[0], x[1]))\n",
    "\n",
    "top_10 = sorted_mapreduce.take(10)\n",
    "\n",
    "for item in top_10:\n",
    "    print(f\"{item[0]}\\t{item[1]}\\t{item[2]}\\t{item[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc25da8-7f99-429d-8938-7d8b2bc562cf",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "## Przetwarzanie - etap 2 - \"Hive\" \n",
    "\n",
    "### Przetwarzanie\n",
    "\n",
    "Twoim zadaniem jest utworzenie obiektu RDD o nazwie `hive_RDD`, który w oparciu o: \n",
    "\n",
    "- utworzony powyżej `mapreduce_RDD` \n",
    "- drugi ze źródłowych RDD (`datasource4`)\n",
    "\n",
    "będzie zawierał wynik zgodny z oczekiwaniami treści z sekcji *Program Hive* dla Twojego zestawu. \n",
    "\n",
    "Wykorzystaj do tego celu poniższe paragrafy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4821e-b1ad-4bbe-b98c-2b4e3f891632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Etap 2: Hive - łączenie danych i agregacja na poziomie krajów i rodzajów kuchni\n",
    "\n",
    "# Parsowanie danych restaurants z wczesną projekcją\n",
    "def parse_restaurants(line):\n",
    "\n",
    "    fields = line.split(',')\n",
    "    \n",
    "    if len(fields) < 5:\n",
    "        return None\n",
    "    \n",
    "    restaurant_id = fields[0].strip().strip('\"')\n",
    "    country = fields[3].strip().strip('\"')\n",
    "    cuisine = fields[4].strip().strip('\"')\n",
    "    \n",
    "    # Wczesna projekcja\n",
    "    return (restaurant_id, (country, cuisine))\n",
    "\n",
    "# Stosujemy parsowanie\n",
    "parsed_restaurants = restaurants_rdd \\\n",
    "    .map(parse_restaurants) \\\n",
    "    .filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af00c41-02a9-4a85-b3c6-bc41098edbe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Klucz: restaurant_id, Wartość: (payment_type, orders_count, avg_total_price)\n",
    "mapreduce_for_join = mapreduce_RDD.map(\n",
    "    lambda x: (x[0], (x[1], x[2], x[3]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b370100-3b56-4668-bb26-3db00b8ad04b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join: łączymy dane zamówień z informacjami o restauracjach\n",
    "\n",
    "joined_data = mapreduce_for_join.join(parsed_restaurants)\n",
    "\n",
    "country_cuisine_data = joined_data.map(\n",
    "    lambda x: (\n",
    "        (x[1][1][0], x[1][1][1]),  # Klucz: (country, cuisine)\n",
    "        (x[1][0][1], x[1][0][1] * x[1][0][2])  # Wartość: (orders_count, total_price_sum)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Agregacja na poziomie (country, cuisine)\n",
    "def sum_aggregator(a, b):\n",
    "    \"\"\"Sumuje (orders_count, total_price_sum)\"\"\"\n",
    "    return (a[0] + b[0], a[1] + b[1])\n",
    "\n",
    "aggregated_by_country_cuisine = country_cuisine_data.reduceByKey(sum_aggregator)\n",
    "\n",
    "# Obliczenie średniej i przygotowanie do rankingu\n",
    "country_cuisine_stats = aggregated_by_country_cuisine.map(\n",
    "    lambda x: (\n",
    "        x[0][0],  # country\n",
    "        (x[0][1], x[1][0], round(x[1][1] / x[1][0], 6))  # (cuisine, total_orders, avg_total_price)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Grupowanie po kraju dla obliczenia rankingu\n",
    "grouped_by_country = country_cuisine_stats.groupByKey()\n",
    "\n",
    "# Obliczanie rankingu w ramach każdego kraju\n",
    "def add_ranking(country_data):\n",
    "    \"\"\"\n",
    "    Dodaje ranking dla kuchni w ramach kraju\n",
    "    country_data: (country, [(cuisine, total_orders, avg_total_price), ...])\n",
    "    \"\"\"\n",
    "    country = country_data[0]\n",
    "    cuisines = list(country_data[1])\n",
    "    \n",
    "    # Sortujemy według total_orders malejąco\n",
    "    sorted_cuisines = sorted(cuisines, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Dodajemy ranking\n",
    "    result = []\n",
    "    for rank, cuisine_data in enumerate(sorted_cuisines, start=1):\n",
    "        result.append((\n",
    "            country,\n",
    "            cuisine_data[0],\n",
    "            cuisine_data[1],\n",
    "            cuisine_data[2],\n",
    "            rank\n",
    "        ))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Stosujemy ranking\n",
    "hive_RDD = grouped_by_country.flatMap(add_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef66612b-909e-412d-9148-4cf588774ff9",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "### Zapis wyniku \n",
    "\n",
    "Mając obiekt RDD o nazwie `hive_RDD` dokonaj jego zapisu w docelowym miejscu określonym w opisie projektu dla tej części projektu.\n",
    "\n",
    "*Uwaga!* uwzględnij fakt, że docelowe miejsce może być \"zajęte\". \n",
    "Przed utworzeniem nowego wyniku usuń ewentualne efekty poprzednich uruchomień notatnika.  \n",
    "Zwróć uwagę, że efekty te mogą istnieć na wielu poziomach.\n",
    "\n",
    "#### Usunięcie poprzednich wyników\n",
    "\n",
    "Wykonaj operacje usuwające ewentualne poprzednio utworzone wyniki.\n",
    "\n",
    "Wykorzystaj do tego celu poniższe paragrafy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fecf1b2-237a-4dc4-9cc7-adc2e806fa4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Usuwamy poprzednie wyniki (jeśli istnieją)\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    subprocess.run(\n",
    "        [\"hadoop\", \"fs\", \"-rm\", \"-r\", \"/tmp/output1\"],\n",
    "        stderr=subprocess.DEVNULL,\n",
    "        check=False\n",
    "    )\n",
    "    print(\"Poprzednie wyniki usunięte\")\n",
    "except:\n",
    "    print(\"Brak poprzednich wyników do usunięcia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c97f70",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "#### Zapis nowe wyniki\n",
    "\n",
    "Dokonaj zapisu nowego wyniku.\n",
    "\n",
    "Wykorzystaj do tego celu poniższe paragrafy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a40af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Zapisujemy wynik w formacie Pickle\n",
    "hive_RDD.saveAsPickleFile(\"/tmp/output1\")\n",
    "print(\"Wyniki zapisane w /tmp/output1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae0494-3a28-4d7e-95f0-b6ca7eee9f8f",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "### Wynik \n",
    "\n",
    "Odczytaj do zmiennej `result_RDD` dane z docelowego miejsca, w którym została zapisana zawartość `hive_RDD`.\n",
    "\n",
    "Wykorzystaj do tego celu poniższe paragrafy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63155041-12fd-4657-89e9-b1b212afc545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wczytujemy wynik z /tmp/output1 (format Pickle)\n",
    "result_RDD = sc.pickleFile(\"/tmp/output1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb7d53c-e271-48fb-8357-811a40143c01",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "Pobierz liczbę elementów z `result_RDD`. \n",
    "\n",
    "Wykorzystaj do tego celu poniższy paragraf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60bc77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Liczba wyników\n",
    "result_count = result_RDD.count()\n",
    "print(f\"Liczba wyników: {result_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb1bb1a",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "Posortuj rosnąco elementy `result_RDD` względem wszystkich wymaganych wynikowych atrybutów (w kolejności określonej w treści sekcji *Program Hive* dla Twojego zestawu). \n",
    "\n",
    "A następnie pobierz i wyświetl w oddzielnych liniach 10 pierwszych elementów. \n",
    "\n",
    "Wykorzystaj do tego celu poniższy paragraf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b1199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sortujemy według wszystkich atrybutów\n",
    "sorted_result = result_RDD.sortBy(lambda x: (x[0], x[1], x[2], x[3], x[4]))\n",
    "\n",
    "top_10_results = sorted_result.take(10)\n",
    "\n",
    "for item in top_10_results:\n",
    "    print(f\"{item[0]}\\t{item[1]}\\t{item[2]}\\t{item[3]}\\t{item[4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa58fe-cea7-4e62-940d-1169b2e1cad5",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "# Część 2 - Spark SQL (*DataFrame API*)\n",
    "\n",
    "Pamiętaj o czystości Twojego API. Musi być ona zachowana od samego początku implementacji (utworzenia kontekstu), poprzez definicje źródeł i transformacje, aż do samego końca czyli wygenerowanie ostatecznego wyniku. \n",
    "\n",
    "Wykorzystaj odpowiedni kontekst utworzony w początkowej części notatnika."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c85b6-5111-49d9-8b48-c2008347949c",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "## Dane źródłowe\n",
    "\n",
    "W poniższych paragrafach utwórz dwa obiekty `DataFrame`, których zawartością będą Twoje dane źródłowe. \n",
    "\n",
    "Użyj nazw czytelnie odnoszących się zawartości Twoich danych źródłowych. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8c395-e13e-4de1-8380-a6e19bb25cbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wczytanie danych orders z datasource1 jako DataFrame\n",
    "orders_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(f\"{input_dir}/datasource1/*.csv\")\n",
    "\n",
    "# Sprawdzenie schematu\n",
    "print(\"Schema orders_df:\")\n",
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed17f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wczytanie danych restaurants z datasource4 jako DataFrame\n",
    "restaurants_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(f\"{input_dir}/datasource4/restaurants.csv\")\n",
    "\n",
    "# Sprawdzenie schematu\n",
    "print(\"Schema restaurants_df:\")\n",
    "restaurants_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c875c26-f0f9-42bb-b06a-50e0f9df44ae",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "## Przetwarzanie - etap 1 - \"MapReduce\" \n",
    "\n",
    "### Przetwarzanie\n",
    "\n",
    "Twoim zadaniem jest utworzenie obiektu `DataFrame` o nazwie `mapreduce_DF`, który w oparciu o: \n",
    "\n",
    "- utworzony kontekst, oraz\n",
    "- pierwszy ze źródłowych `DataFrame` (`datasource1`)\n",
    "\n",
    "będzie zawierał wynik zgodny z oczekiwaniami treści sekcji *Program MapReduce* dla Twojego zestawu. \n",
    "\n",
    "Wykorzystaj do tego celu poniższe paragrafy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c54269d-1ebc-44d4-9a38-65f5a5e490dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Etap 1: MapReduce - statystyki zamówień dla restauracji i typów płatności\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Wczesna selekcja i projekcja\n",
    "mapreduce_DF = orders_df \\\n",
    "    .filter(F.col(\"status\") == \"Completed\") \\\n",
    "    .select(\"restaurant_id\", \"payment_type\", \"total_price_usd\") \\\n",
    "    .groupBy(\"restaurant_id\", \"payment_type\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"orders_count\"),\n",
    "        F.round(F.avg(\"total_price_usd\"), 6).alias(\"avg_total_price\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae8d1c-0739-47e0-aa9e-cd4db027590b",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "### Wynik\n",
    "\n",
    "Pobierz liczbę elementów z `mapreduce_DF`. \n",
    "\n",
    "Wykorzystaj do tego celu poniższy paragraf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff10e8f9-b751-472f-ab65-f5a58532d053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapreduce_df_count = mapreduce_DF.count()\n",
    "print(f\"Liczba grup: {mapreduce_df_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20c5c4",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "Posortuj rosnąco elementy `mapreduce_DF` względem wszystkich wymaganych wynikowych atrybutów (w kolejności określonej w treści sekcji *Program MapReduce* dla Twojego zestawu). \n",
    "\n",
    "A następnie pobierz i wyświetl w oddzielnych liniach 10 pierwszych elementów. Skorzystaj z metody `show()`.\n",
    "\n",
    "Wykorzystaj do tego celu poniższy paragraf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b5583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sortujemy według wszystkich atrybutów: restaurant_id, payment_type\n",
    "mapreduce_DF \\\n",
    "    .orderBy(\"restaurant_id\", \"payment_type\") \\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae198e7-b48d-4efa-84c7-49e3c6822cac",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "## Przetwarzanie - etap 2 - \"Hive\" \n",
    "\n",
    "### Przetwarzanie\n",
    "\n",
    "Twoim zadaniem jest utworzenie obiektu `DataFrame`o nazwie `hive_DF`, który w oparciu o: \n",
    "\n",
    "- utworzony powyżej `mapreduce_DF` \n",
    "- drugi ze źródłowych `DataFrame` (`datasource4`)\n",
    "\n",
    "będzie zawierał wynik zgodny z oczekiwaniami treści z sekcji *Program Hive* dla Twojego zestawu. \n",
    "\n",
    "Wykorzystaj do tego celu poniższe paragrafy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb273669-44f8-46a7-9ece-0d3f22a8d304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Etap 2: Hive - łączenie danych i agregacja na poziomie krajów i rodzajów kuchni\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Wczesna projekcja\n",
    "restaurants_selected = restaurants_df.select(\"restaurant_id\", \"country\", \"cuisine\")\n",
    "\n",
    "joined_df = mapreduce_DF.join(restaurants_selected, \"restaurant_id\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac2255-2b52-474c-bfa4-92acd5419c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregacja na poziomie (country, cuisine)\n",
    "country_cuisine_agg = joined_df \\\n",
    "    .groupBy(\"country\", \"cuisine\") \\\n",
    "    .agg(\n",
    "        F.sum(\"orders_count\").alias(\"total_orders\"),\n",
    "        F.round(\n",
    "            F.sum(F.col(\"orders_count\") * F.col(\"avg_total_price\")) / F.sum(\"orders_count\"),\n",
    "            6\n",
    "        ).alias(\"avg_total_price\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4188ac5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dodawanie rankingu w ramach każdego kraju\n",
    "\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(F.col(\"total_orders\").desc())\n",
    "\n",
    "hive_DF = country_cuisine_agg \\\n",
    "    .withColumn(\"rank_in_country\", F.rank().over(window_spec))\n",
    "\n",
    "hive_DF = hive_DF.select(\n",
    "    \"country\",\n",
    "    \"cuisine\", \n",
    "    \"total_orders\",\n",
    "    \"avg_total_price\",\n",
    "    \"rank_in_country\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8c7ac1-f774-4efc-9351-855c5ad232ae",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "### Zapis wyniku \n",
    "\n",
    "Mając obiekt `DataFrame` o nazwie `hive_DF` dokonaj jego zapisu w docelowym miejscu określonym w opisie projektu dla tej części projektu.\n",
    "\n",
    "*Uwaga!* uwzględnij fakt, że docelowe miejsce może być \"zajęte\". \n",
    "Przed utworzeniem nowego wyniku usuń ewentualne efekty poprzednich uruchomień notatnika.  \n",
    "Zwróć uwagę, że efekty te mogą istnieć na wielu poziomach.\n",
    "\n",
    "#### Usunięcie poprzednich wyników\n",
    "\n",
    "Wykonaj operacje usuwające ewentualne poprzednio utworzone wyniki.\n",
    "\n",
    "Wykorzystaj do tego celu poniższe paragrafy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8607552a-f029-4216-8f2c-77bb76cbcd00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Usuwamy katalog z danymi Delta Lake\n",
    "try:\n",
    "    subprocess.run(\n",
    "        [\"hadoop\", \"fs\", \"-rm\", \"-r\", \"/tmp/delta/output2\"],\n",
    "        stderr=subprocess.DEVNULL,\n",
    "        check=False\n",
    "    )\n",
    "    print(\"Poprzednie wyniki Delta Lake usunięte\")\n",
    "except:\n",
    "    print(\"Brak poprzednich wyników do usunięcia\")\n",
    "\n",
    "# Usuwamy tabelę z metastore (jeśli istnieje)\n",
    "try:\n",
    "    spark.sql(\"DROP TABLE IF EXISTS output2\")\n",
    "    print(\"Tabela output2 usunięta z metastore\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7c8ef9",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "#### Zapis nowe wyniki\n",
    "\n",
    "Dokonaj zapisu nowego wyniku.\n",
    "\n",
    "Wykorzystaj do tego celu poniższe paragrafy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d23d88d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Zapisujemy DataFrame w formacie Delta Lake\n",
    "hive_DF.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/tmp/delta/output2\")\n",
    "\n",
    "print(\"Wyniki zapisane w Delta Lake: /tmp/delta/output2\")\n",
    "\n",
    "# Opcjonalnie: rejestrujemy jako tabelę\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS output2\n",
    "    USING DELTA\n",
    "    LOCATION '/tmp/delta/output2'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Tabela output2 zarejestrowana w metastore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66e5d6a-958b-4927-9df1-40564743702a",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "### Wynik \n",
    "\n",
    "Odczytaj do zmiennej `result_DF` dane z docelowego miejsca, w którym została zapisana zawartość `hive_DF`.\n",
    "\n",
    "Wykorzystaj do tego celu poniższe paragrafy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa89de-3be2-40a2-872b-1b0f0f47d7bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wczytujemy wynik z Delta Lake\n",
    "result_DF = spark.read.format(\"delta\").load(\"/tmp/delta/output2\")\n",
    "\n",
    "# Lub alternatywnie z tabeli:\n",
    "# result_DF = spark.table(\"output2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7854f00-f81f-4e86-bb24-72c3828df17a",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "Pobierz liczbę elementów z `result_DF`. \n",
    "\n",
    "Wykorzystaj do tego celu poniższy paragraf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53097d70-8d7c-4494-977e-8e43e9b4b4be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Liczba wyników\n",
    "result_df_count = result_DF.count()\n",
    "print(f\"Liczba wyników: {result_df_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96055677-66ac-4bf2-8f75-707cda42959b",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "Posortuj rosnąco elementy `result_DF` względem wszystkich wymaganych wynikowych atrybutów (w kolejności określonej w treści sekcji *Program Hive* dla Twojego zestawu). \n",
    "\n",
    "A następnie pobierz i wyświetl w oddzielnych liniach 10 pierwszych elementów. \n",
    "\n",
    "Wykorzystaj do tego celu poniższy paragraf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65027ec2-f755-4e0b-8051-294c4f878cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sortujemy według wszystkich atrybutów\n",
    "result_DF \\\n",
    "    .orderBy(\"country\", \"cuisine\", \"total_orders\", \"avg_total_price\", \"rank_in_country\") \\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52cfeae-b2dc-4123-a89c-1a5106058b48",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "## Zamknięcie kontekstu \n",
    "\n",
    "Zamknij kontekst, wyłączając w ten sposób aplikację *Apache Spark*.\n",
    "\n",
    "Wykorzystaj do tego celu poniższy paragraf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307b2ad-85a8-4303-9c06-a299bbf320db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Sesja Spark zamknięta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce6533",
   "metadata": {},
   "source": [
    "> Instrukcja wykonania projektu\n",
    "\n",
    "# Podsumowanie \n",
    "\n",
    "Jeśli implementacja tego notatnika została przez Ciebie zakończona, koniecznie uruchom jego całość kilkukrotnie, aby upewnić się, że całość przetwarzania jest poprawna, a samo przetwarzanie jest powtarzalne. \n",
    "\n",
    "Na zakończenie, przed rejestracją tego notatnika w ramach projektu, pamiętaj o:\n",
    "\n",
    "- usunięciu wartości zmiennej `input_dir`\n",
    "- wyczyszczeniu wszystkich wyników (prawy klawisz myszy i pozycja *Clear Output of All Cells*). <br>\n",
    "  Pozostawienie Twoich wyników może być potraktowane jako chęć wpływania na ocenę recenzentów\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe3255b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
